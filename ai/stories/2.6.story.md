

File: `ai/stories/2.6.story.md`

# Story 2.6: Implement Text-to-Speech (TTS) for iOS via KMP Abstraction

**Status:** Draft

## Goal & Context

**User Story:** As a User, after looking up a word in the iOS Action Extension, I want to be able to tap a button to hear its pronunciation in a US English accent, using the KMP `TtsPlayer` abstraction.

**Context:** This story mirrors Story 2.5 for iOS. It activates the TTS placeholder button from Story 2.4 (iOS Action Extension UI). It requires the full `actual` implementation of the `TtsPlayer` for iOS (defined as `expect` in Story 1.7 and Tech Story S.3) using iOS's native `AVSpeechSynthesizer`.

## Detailed Requirements

- The `iosApp` module (or the `shared/src/iosMain/.../platform/`) implements the `actual TtsPlayer` for iOS (e.g., `IosTtsPlayer.kt`), wrapping iOS's native `AVSpeechSynthesizer`. This `actual` implementation should be injectable via Koin.
- Configure TTS for US English accent (e.g., `AVSpeechSynthesisVoice(language: "en-US")`).
- When the pronunciation placeholder (from Story 2.4) is tapped, the iOS Action Extension UI invokes the `speak` method of the KMP `TtsPlayer` instance (likely obtained via the `ActionViewModel`).
- Handle TTS engine initialization and potential errors within the `actual` implementation.

## Acceptance Criteria (ACs)

- AC1: Tapping the pronunciation button in the iOS Action Extension UI triggers the `speak` method of the KMP `TtsPlayer`.
- AC2: The `IosTtsPlayer` (actual KMP implementation) uses `AVSpeechSynthesizer` to pronounce the word with a US English accent.
- AC3: The app correctly handles TTS initialization (AVSpeechSynthesizer doesn't have an explicit async init like Android's, but voice availability can be checked).

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Refer to the linked `docs/` files for broader context if needed.

- **Relevant Files:**

    - Files to Create/Modify:
        - `shared/src/iosMain/kotlin/com/nguyenmoclam/sublingo/shared/platform/IosTtsPlayer.kt` (Full implementation of `actual TtsPlayer`)
        - `iosApp/SubLingoActionExtension/ActionViewModel.swift` (To hold `TtsPlayer` instance and expose speak function)
        - `iosApp/SubLingoActionExtension/ActionView.swift` (To connect button's action to ViewModel)
        - Koin modules to provide `IosTtsPlayer`.
    - *(Hint: See `docs/project-structure.md` and Story 1.7 for `expect`/`actual` TtsPlayer setup.)*

- **Key Technologies:**

    - Kotlin (for `actual` implementation), Swift (for ViewModel and UI)
    - iOS SDK, `AVFoundation` framework (`AVSpeechSynthesizer`, `AVSpeechUtterance`, `AVSpeechSynthesisVoice`)
    - KMP `expect`/`actual`.
    - Koin (for DI).
    - Swift Concurrency / Combine (ViewModel).
    - *(Hint: See `docs/tech-stack.md`)*

- **API Interactions / SDK Usage:**

    - `AVSpeechSynthesizer`, `AVSpeechUtterance(string: text)`, `AVSpeechSynthesisVoice(language: "en-US")`.
    - The `expect class TtsPlayer` from Story 1.7.

- **Data Structures:**

    - Not applicable.

- **Environment Variables:**

    - Not applicable.

- **Coding Standards Notes:**

    - Manage the `AVSpeechSynthesizer` instance.
    - Ensure voice and language settings are correctly applied.
    - *(Hint: See `docs/coding-standards.md`)*

## Tasks / Subtasks

- [ ] Task 1: Fully implement `actual class IosTtsPlayer` in `shared/src/iosMain/.../platform/`.
    - [ ] Subtask 1.1: Create an instance of `AVSpeechSynthesizer`.
    - [ ] Subtask 1.2: Implement `initialize(onResult: (success: Boolean) -> Unit)`:
        - Check if `AVSpeechSynthesisVoice(language: "en-US")` is available. Call `onResult` accordingly. (iOS TTS doesn't have an async init like Android, but this check is good).
    - [ ] Subtask 1.3: Implement `speak(text: String)`:
        - Create an `AVSpeechUtterance` with the given text.
        - Set its `voice` to `AVSpeechSynthesisVoice(language: "en-US")`.
        - Call `synthesizer.speak(utterance)`.
- [ ] Task 2: Update Koin modules to provide `IosTtsPlayer` where `TtsPlayer` is requested for iOS targets.
- [ ] Task 3: Modify `ActionViewModel.swift` (`iosApp/SubLingoActionExtension/`).
    - [ ] Subtask 3.1: Inject/obtain `TtsPlayer`.
    - [ ] Subtask 3.2: Call `ttsPlayer.initialize()` (e.g., in `init`), observe result.
    - [ ] Subtask 3.3: Expose `pronounceWord(word: String)` function calling `ttsPlayer.speak(word)`.
    - [ ] Subtask 3.4: Expose TTS availability if needed for UI.
- [ ] Task 4: Update `ActionView.swift` (`iosApp/SubLingoActionExtension/`).
    - [ ] Subtask 4.1: Enable the TTS `Button`.
    - [ ] Subtask 4.2: On tap, call `viewModel.pronounceWord(wordDefinition.word)`.
    - [ ] Subtask 4.3: Optionally, disable button if TTS unavailable.
- [ ] Task 5: Test TTS functionality:
    - [ ] Subtask 5.1: Tap pronunciation button in Action Extension. Verify `IosTtsPlayer.speak` called (AC1).
    - [ ] Subtask 5.2: Listen for audio. Verify US English accent (AC2).
    - [ ] Subtask 5.3: Verify voice availability check during init (AC3).

## Testing Requirements

**Guidance:** Verify implementation against the ACs using the following tests.

- **Unit Tests:**
    - Unit test `ActionViewModel` TTS interaction by mocking `TtsPlayer`.
- **Integration Tests:** Not primary for MVP.
- **Manual/CLI Verification:**
    - AC1: Use debugger/logging.
    - AC2: Manually listen to pronunciation on an iOS device/simulator with audio.
    - AC3: Check logs for voice availability.
- *(Hint: See `docs/testing-strategy.md`)*

## Story Wrap Up (Agent Populates After Execution)

- **Agent Model Used:** `<Agent Model Name/Version>`
- **Completion Notes:** {}
- **Change Log:**
    - Initial Draft
