
# Story 3.2: Implement OCR Text Extraction on Android (ML Kit)

**Status:** Draft

## Goal & Context

**User Story:** As a Developer, I need to integrate ML Kit on Android (via `MlKitOcrProvider.kt`) to perform OCR on a provided (cropped) image, so that text can be extracted from Reddit comments.

**Context:** This story implements the core OCR functionality on Android using Google's ML Kit. It takes the cropped `Bitmap` from Story 3.1, processes it to extract text, and makes this text available for translation. This is a key step in enabling translation of image-based comments.

## Detailed Requirements

- Use Google's ML Kit Text Recognition on-device API (`com.google.android.gms:play-services-mlkit-text-recognition`).
- Create a service/provider class (e.g., `MlKitOcrProvider`) in `composeApp` that accepts an Android `Bitmap` and returns the recognized text as a string (or a structured result with confidence scores if available).
- Handle potential ML Kit initialization and processing errors.
- Focus on extracting English text. The result will be sent to the KMP `shared` module.

## Acceptance Criteria (ACs)

- AC1: The `MlKitOcrProvider` successfully processes a sample screenshot of a Reddit comment and extracts text.
- AC2: The extracted text is returned as a string.
- AC3: Errors during OCR processing (e.g., no text found, low confidence) are handled gracefully and can be reported (e.g., return null, empty string, or an error state).
- AC4: OCR is performed on-device.

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Refer to the linked `docs/` files for broader context if needed.

- **Relevant Files:**

    - Files to Create:
        - `composeApp/src/androidMain/kotlin/com/nguyenmoclam/sublingo/android/ocr/MlKitOcrProvider.kt`
    - Files to Modify:
        - `composeApp/build.gradle.kts` (to add ML Kit dependency)
        - `composeApp/src/androidMain/kotlin/com/nguyenmoclam/sublingo/android/service/FloatingBubbleService.kt` (to call `MlKitOcrProvider`)
    - *(Hint: See `docs/project-structure.md` for proposed file locations.)*

- **Key Technologies:**

    - Kotlin, Android SDK
    - Google ML Kit Text Recognition (`com.google.android.gms:play-services-mlkit-text-recognition`)
    - `InputImage.fromBitmap()`
    - *(Hint: See `docs/tech-stack.md`)*

- **API Interactions / SDK Usage:**

    - `TextRecognition.getClient(TextRecognizerOptions.DEFAULT_OPTIONS)`
    - `recognizer.process(image)`
    - Handling `addOnSuccessListener` and `addOnFailureListener` for the processing task.
    - Iterating through `Text.getTextBlocks()` to assemble the recognized string.

- **Data Structures:**

    - Input: `android.graphics.Bitmap`
    - Output: `String` (extracted text)

- **Environment Variables:**

    - Not applicable for this story.

- **Coding Standards Notes:**

    - `MlKitOcrProvider` should encapsulate all ML Kit interaction logic.
    - OCR processing should be asynchronous to avoid blocking the calling thread. Use Kotlin Coroutines (`suspendCancellableCoroutine` or similar to wrap ML Kit's Task API).
    - Ensure ML Kit models are downloaded (usually automatic, but be aware of initial download behavior).
    - *(Hint: See `docs/coding-standards.md`)*

## Tasks / Subtasks

- [ ] Task 1: Add ML Kit Text Recognition dependency to `composeApp/build.gradle.kts`.
  ```gradle
  // dependencies {
  //     implementation("com.google.android.gms:play-services-mlkit-text-recognition:19.0.0") // Or latest
  // }
  ```
    - Consider adding the model to `AndroidManifest.xml` to have it available when app is installed if desired:
  <!-- end list -->
  ```xml
  // <application ...>
  //   <meta-data
  //       android:name="com.google.mlkit.vision.DEPENDENCIES"
  //       android:value="ocr" />
  // </application>
  ```
- [ ] Task 2: Implement `MlKitOcrProvider.kt`.
    - [ ] Subtask 2.1: Create a class `MlKitOcrProvider`.
    - [ ] Subtask 2.2: Implement a public function `suspend fun recognizeText(bitmap: Bitmap): String?` (or a Result type).
    - [ ] Subtask 2.3: Inside the function, get a `TextRecognizer` instance.
    - [ ] Subtask 2.4: Create an `InputImage` from the bitmap: `InputImage.fromBitmap(bitmap, 0)`.
    - [ ] Subtask 2.5: Call `recognizer.process(inputImage)` and await its result using `suspendCancellableCoroutine` or by converting the Task to a Coroutine.
    - [ ] Subtask 2.6: On success, iterate through the `Text` result (blocks, lines, elements) to build the full string of recognized text.
    - [ ] Subtask 2.7: On failure or if no text is found, return `null` or an appropriate error indicator.
- [ ] Task 3: In `FloatingBubbleService.kt`, after obtaining the cropped bitmap (from Story 3.1), call `mlKitOcrProvider.recognizeText(croppedBitmap)`.
- [ ] Task 4: Test with a sample Reddit comment screenshot (AC1).
    - [ ] Subtask 4.1: Manually pass a test `Bitmap` to `MlKitOcrProvider`.
    - [ ] Subtask 4.2: Log or debug the extracted text.
- [ ] Task 5: Verify extracted text is a string (AC2).
- [ ] Task 6: Test error handling (e.g., process an image with no text, or simulate a failure) (AC3).
- [ ] Task 7: Confirm that ML Kit is performing on-device OCR (AC4 - this is default for `play-services-mlkit-text-recognition` unless cloud-based variant explicitly chosen, which is not the case here).

## Testing Requirements

**Guidance:** Verify implementation against the ACs using the following tests.

- **Unit Tests:**
    - Unit testing `MlKitOcrProvider` directly can be hard due to ML Kit dependencies. If possible, mock `TextRecognizer` and `Task` to test the logic of success/failure handling and text assembly.
- **Integration Tests:** Not for MVP.
- **Manual/CLI Verification:**
    - AC1: Provide a clear screenshot of a Reddit comment to the provider. Log the output. Compare with actual text.
    - AC2: Check the type and content of the returned string.
    - AC3: Provide a blank image or an image with non-textual elements. Check for graceful handling (e.g., null or empty string returned, no crash).
    - AC4: Assumed by using the on-device ML Kit library.
- *(Hint: See `docs/testing-strategy.md`. Specialized OCR accuracy testing is in Story 6.6.)*

## Story Wrap Up (Agent Populates After Execution)

- **Agent Model Used:** `<Agent Model Name/Version>`
- **Completion Notes:** {}
- **Change Log:**
    - Initial Draft
