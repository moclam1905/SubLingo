
# Story 3.6: Display OCR-Extracted and Translated Text in iOS Share Extension UI

**Status:** Draft

## Goal & Context

**User Story:** As a User, after sharing a screenshot of a Reddit comment to the SubLingo iOS Share Extension, I want to see the extracted text (or its translation).

**Context:** This story closes the OCR loop on iOS, similar to Story 3.3 for Android. It takes the text extracted by `VisionOcrProvider` (Story 3.5) and displays it within the Share Extension's SwiftUI UI. It involves passing the text to the KMP module for initial processing (e.g., first-word offline lookup). Full sentence translation is deferred to Epic 4.

## Detailed Requirements

- After `VisionOcrProvider` extracts text (Story 3.5), the `ShareViewController.swift` passes this text to the KMP `shared` module (specifically to a domain use case like `ProcessExtractedTextUseCase` via the `ActionViewModel` or similar ObservableObject).
- The KMP use case will decide whether to use `OfflineDictionaryService` (for single words) or `OnlineTranslationService` (for sentences, if enabled).
- For this story, if the extracted text is multiple words, initially display the raw extracted text or the offline translation of the first recognized word. Full sentence translation display will align with Epic 4.
- Update the Share Extension's SwiftUI UI to show the OCR result/translation.
- Provide user feedback during OCR processing (e.g., a loading indicator in the extension UI).

## Acceptance Criteria (ACs)

- AC1: After sharing an image and OCR, the iOS Share Extension UI displays the extracted text or the offline translation of the first word.
- AC2: A loading indicator is shown while OCR/translation is in progress within the extension.
- AC3: If OCR fails to extract text, an appropriate "No text found" message is shown in the extension UI.

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Refer to the linked `docs/` files for broader context if needed.

- **Relevant Files:**

    - Files to Modify:
        - `iosApp/SubLingoShareExtension/ShareViewController.swift` (Orchestrate OCR call and update ViewModel)
        - `iosApp/SubLingoShareExtension/ActionViewModel.swift` (or a new `ShareViewModel.swift` if logic differs significantly from Action Extension; to handle OCR result and call KMP)
        - `iosApp/SubLingoShareExtension/ShareView.swift` (or `ActionView.swift`; SwiftUI to display result/loading)
    - *(Hint: See `docs/project-structure.md`.)*

- **Key Technologies:**

    - Swift, SwiftUI, iOS App Extension (Share)
    - Koin (for KMP services in ViewModel)
    - Combine or Swift Concurrency (`async/await`)
    - `OfflineDictionaryService` (from KMP, Story 2.2)
    - *(Hint: See `docs/tech-stack.md`)*

- **API Interactions / SDK Usage:**

    - ViewModel calls KMP services.
    - `VisionOcrProvider.recognizeText()` output is input here.

- **Data Structures:**

    - Input: `String` (OCR'd text).
    - Output: `WordDefinition` (if first word lookup) or raw string.
    - ViewModel state needs to handle OCR results.

- **Environment Variables:**

    - Not applicable.

- **Coding Standards Notes:**

    - Update ViewModel to manage OCR state (loading, result, error).
    - SwiftUI UI should reactively display these states.
    - *(Hint: See `docs/coding-standards.md`)*

## Tasks / Subtasks

- [ ] Task 1: Enhance `ActionViewModel.swift` (or create `ShareViewModel.swift` if preferred for Share Extension).
    - [ ] Subtask 1.1: Add state properties for OCR loading, raw OCR text, and processed result.
    - [ ] Subtask 1.2: Create/update `processOcrText(ocrText: String?)` function (similar to Android's ViewModel).
        - If `ocrText` is null/empty, update state to show "No text found."
        - If `ocrText` has content:
            - For this story: Extract first word. Call KMP `offlineDictionaryRepository.findWord(firstWord)` (or use case).
            - Update UI state with loading, then with result.
- [ ] Task 2: Modify `ShareViewController.swift`.
    - [ ] Subtask 2.1: After `VisionOcrProvider` returns text, call `viewModel.processOcrText(extractedText)`.
    - [ ] Subtask 2.2: Trigger loading state in ViewModel before OCR.
- [ ] Task 3: Update Share Extension's SwiftUI UI (`ShareView.swift` or `ActionView.swift`).
    - [ ] Subtask 3.1: Display loading indicator based on ViewModel state.
    - [ ] Subtask 3.2: If OCR fails, display "No text found."
    - [ ] Subtask 3.3: If OCR succeeds, display first word definition or raw extracted text.
- [ ] Task 4: Test end-to-end flow: Share screenshot -\> OCR -\> Loading indicator -\> Result or "No text found" in UI. (AC1, AC2, AC3).

## Testing Requirements

**Guidance:** Verify implementation against the ACs using the following tests.

- **Unit Tests:**
    - Test ViewModel logic for `processOcrText` (mock OCR input, KMP service, verify state).
- **Integration Tests:** Not for MVP.
- **Manual/CLI Verification:**
    - AC1: Share image with comment. Verify extracted text (or first word def) appears.
    - AC2: Observe loading indicator.
    - AC3: Share image with no text. Verify "No text found."
- *(Hint: See `docs/testing-strategy.md`)*

## Story Wrap Up (Agent Populates After Execution)

- **Agent Model Used:** `<Agent Model Name/Version>`
- **Completion Notes:** {}
- **Change Log:**
    - Initial Draft
